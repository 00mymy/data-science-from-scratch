{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "* Classification & Regression\n",
    "* 대표적인 Black-box 모델\n",
    "  + cf) Decision Tree\n",
    "* Brain-like\n",
    "  + Network of neurons\n",
    "  + Neuron들 사이의 연결강도(weight)에 의해 모델이 결정됨\n",
    "  + 어렵고 복잡한 문제도 잘 푼다 \n",
    "      - 복잡한 문제 &rarr; 더 많은 neuron &rarr; 더 많은 계산비용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "* Neural Network을 구성하는 기본 단위\n",
    "* Simple, single neuron (뇌세포 하나에 해당)\n",
    "* $y = f(\\sum x_iw_i$)\n",
    "    + $f$ : Step function\n",
    "    + $x$ : Inputs\n",
    "    + $w$ : weights ($w_0$ : weight for bias) \n",
    "\n",
    "![Perceptron](images/perceptron.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scratch.linear_algebra import Vector, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron으로 간단한 문제 풀기\n",
    "* Logic Gates\n",
    " \n",
    "|x|y|AND(x,y)|OR(x,y)|NOT(x)|XOR(x,y)|\n",
    "|---|---|---|---|---|---|\n",
    "|0|0|0|0|1|0|\n",
    "|0|1|0|1|1|1|\n",
    "|1|0|0|1|0|1|\n",
    "|1|1|1|1|0|0|\n",
    "\n",
    "* AND, OR, NOT Gate를 Perceptron으로 풀기\n",
    "* 문제풀기 = Weight 찾기\n",
    "\n",
    "<img src=\"images/simple_gates.png\" alt=\"Simple Gates\" style=\"width:600px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
    "\n",
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n",
    "\n",
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR gate?\n",
    "* (0,1), (1,0) &rarr; 1\n",
    "* (0,0), (1,1) &rarr; 0\n",
    "\n",
    "생각처럼 쉽지 않음\n",
    "* 2차원 평면 좌표계 : XOR 값을 구별할 수 있는 하나의 직선(분할기준)을 구할 수 없다\n",
    "* 즉, 하나의 Perceptron만으로는 XOR 값을 구별할 수 없다\n",
    "* 해석적으로 풀 수는 있지만...\n",
    "    + XOR(x,y) = OR(x,y) AND (NOT(AND(x,y)))\n",
    "\n",
    "![XOR Gate](images/xor_gate_naive.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "* 여러 개의 Perceptron을 연결하는 체계적인 모형\n",
    "* Multi-layer Perceptron (Layered Network of Neurons, \"Neural Network\")\n",
    "  + Input Layer\n",
    "  + Hidden Layers (여러개)\n",
    "  + Output Layer\n",
    "  + 각 Layer는 여러 개의 Perceptron으로 구성\n",
    "* Activation function : Sigmoid\n",
    "  + S자 모양의 함수\n",
    "  + $f(x)={1\\over{(1+\\exp^{-x})}}$\n",
    "  + 중간영역에서는 Linear하게 증감하지만, 큰 값이나 작은 값으로 갈수록 특정 값으로 수렴하는 특징을 가진다\n",
    "    - 모델이 예측하는 값이 크면 클수록 1이 나올 가능성이 높고, 작아질수록 0이 나올 가능성이 높아지도록 변환시킬 수 있다\n",
    "  + Wiki: [시그모이드 함수](https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%ED%95%A8%EC%88%98)\n",
    "\n",
    "![Feed-Forward Neural Network](images/feed_forward_nn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step Function vs. Sigmoid Function](images/step_vs_sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Network 구성 - XOR Gate\n",
    "* Input Layer : Neuron 2개(입력 데이터 Feature 수 만큼) + bias\n",
    "* Hidden Layer : Neuron 2개\n",
    "    + 왜 2개인지,  Weight를 어떻게 찾았는지는 생략 (찾았다 치고!)\n",
    "* Output Layer : Neuron 1개 (0 또는 1의 값을 얻기 위함)\n",
    "\n",
    "#### Feed-Forward\n",
    "* Weight를 찾은 상태에서 결과값을 얻는 처리 과정\n",
    "\n",
    "![Neural Network for XOR](images/xor_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 0.0\n",
      "[0, 1] 1.0\n",
      "[1, 0] 1.0\n",
      "[1, 1] 0.0\n"
     ]
    }
   ],
   "source": [
    "xor_network = [ # hidden layer\n",
    "            [[20, 20, -30], # 'and' neuron\n",
    "             [20, 20, -10]], # 'or' neuron\n",
    "            # output layer\n",
    "            [[-60, 60, -30]]] # '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001\n",
    "\n",
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "for input in inputs:\n",
    "    print( input, round(feed_forward(xor_network, input)[-1][0],2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer의 역할\n",
    "* \"처리 단계\"와 같은 효과\n",
    "    + 단계를 거치면서 경우의 수를 줄여주는 효과\n",
    "* 복잡한 문제일수록 &rarr; 많은 Hidden Layer, 많은 Neuron &rarr; Weight를 어떻게 찾을까?\n",
    "    + 간단한 XOR Gate 문제도 Weight 찾기가 어려운데...\n",
    "\n",
    "![Hidden Layer의 역할](images/hidden_layer_effect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "* Neural Network의 Weight를 구하는(최적화하는) 기계학습 방법\n",
    "* Weight : 학습된 모델\n",
    "* Weight 구하기 : 기계학습 과정 (반복하면서, 실측치와 예측치를 비교하면서, Gradient Descent를 이용하여 조정)\n",
    "\n",
    "<ol>\n",
    "<li> Run feed_forward on an input vector to produce the outputs of all the neurons in the network.\n",
    "<li> We know the target output, so we can compute a <i>loss</i> that's the sum of the squared errors.\n",
    "<li> Compute the gradient of this error as a function of the neuron’s weights.\n",
    "<li> “Propagate” the gradients and errors backward to compute the gradients with respect to the hidden neurons' weight.\n",
    "<li> Take a gradient descent step.\n",
    "</ol>\n",
    "\n",
    "<img align=\"left\"  src=\"images/backprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "<img align=\"left\"  src=\"images/backprop_ex.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR - Neural Network으로 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|██████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 6941.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]\n",
    "\n",
    "from scratch.gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                   for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0] 0.0\n",
      "[0.0, 1] 1.0\n",
      "[1.0, 0] 1.0\n",
      "[1.0, 1] 0.0\n"
     ]
    }
   ],
   "source": [
    "for input in xs:\n",
    "    print( input, round(predict(xor_network, input)[0],2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Fizz Buzz\n",
    "* 3의 배수: \"fizz\"\n",
    "* 5의 배수: \"buzz\"\n",
    "* 15의 배수: \"fizzbuzz\"\n",
    "* 모형 설계\n",
    "  + 입력: Binary Encoding된 벡터\n",
    "    - 예) 1 &rarr; [1, 1, 0, 0, 0, 0, 0, 0,  0,  0] (거꾸로 배열된 2진수)\n",
    "    - \"거꾸로 배열된 2진수\"라도 상관없음 (각 숫자를 고유의 이미지 벡터로 표현한 것일 뿐)\n",
    "  + 출력: FizzBuzz Encoding된 벡터\n",
    "    - 예) \"fizz\" &rarr; [0, 1, 0, 0]\n",
    "    - 예) \"buzz\" &rarr; [0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]\n",
    "\n",
    "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
    "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
    "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
    "assert fizz_buzz_encode(30) == [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary\n",
    "\n",
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습시키기\n",
    "* 101 ~ 1023 까지의 숫자(입력)와 해당 숫자의 FizzBuzz 값(출력/정답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss: 29.53): 100%|███████████████████████████████████████████████████████| 500/500 [05:26<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]\n",
    "\n",
    "from scratch.linear_algebra import squared_distance\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += squared_distance(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측하기\n",
    "* 1~100까지의 숫자에 대해 FizzBuzz 예측\n",
    "  + 학습할 때 사용하지 않은 숫자에 대해 얼마나 정확하게 예측하는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "\n",
    "assert argmax([0, -1]) == 0               # items[0] is largest\n",
    "assert argmax([-1, 0]) == 1               # items[1] is largest\n",
    "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "2 2 2\n",
      "3 fizz fizz\n",
      "4 4 4\n",
      "5 buzz buzz\n",
      "6 fizz fizz\n",
      "7 7 7\n",
      "8 8 8\n",
      "9 fizz fizz\n",
      "10 buzz buzz\n",
      "11 11 11\n",
      "12 fizz fizz\n",
      "13 13 13\n",
      "14 14 14\n",
      "15 fizzbuzz fizzbuzz\n",
      "16 16 16\n",
      "17 17 17\n",
      "18 fizz fizz\n",
      "19 19 19\n",
      "20 20 buzz\n",
      "21 fizz fizz\n",
      "22 22 22\n",
      "23 23 23\n",
      "24 fizz fizz\n",
      "25 buzz buzz\n",
      "26 26 26\n",
      "27 fizz fizz\n",
      "28 28 28\n",
      "29 29 29\n",
      "30 fizzbuzz fizzbuzz\n",
      "31 31 31\n",
      "32 32 32\n",
      "33 fizz fizz\n",
      "34 34 34\n",
      "35 buzz buzz\n",
      "36 fizz fizz\n",
      "37 37 37\n",
      "38 38 38\n",
      "39 fizz fizz\n",
      "40 buzz buzz\n",
      "41 41 41\n",
      "42 fizz fizz\n",
      "43 43 43\n",
      "44 44 44\n",
      "45 fizzbuzz fizzbuzz\n",
      "46 46 46\n",
      "47 47 47\n",
      "48 fizz fizz\n",
      "49 49 49\n",
      "50 buzz buzz\n",
      "51 fizz fizz\n",
      "52 52 52\n",
      "53 53 53\n",
      "54 fizz fizz\n",
      "55 buzz buzz\n",
      "56 56 56\n",
      "57 fizz fizz\n",
      "58 58 58\n",
      "59 59 59\n",
      "60 fizzbuzz fizzbuzz\n",
      "61 61 61\n",
      "62 62 62\n",
      "63 fizz fizz\n",
      "64 64 64\n",
      "65 buzz buzz\n",
      "66 fizz fizz\n",
      "67 67 67\n",
      "68 68 68\n",
      "69 fizz fizz\n",
      "70 buzz buzz\n",
      "71 71 71\n",
      "72 fizz fizz\n",
      "73 73 73\n",
      "74 74 74\n",
      "75 fizzbuzz fizzbuzz\n",
      "76 76 76\n",
      "77 77 77\n",
      "78 fizz fizz\n",
      "79 79 79\n",
      "80 80 buzz\n",
      "81 fizz fizz\n",
      "82 82 82\n",
      "83 83 83\n",
      "84 fizz fizz\n",
      "85 fizz buzz\n",
      "86 86 86\n",
      "87 fizz fizz\n",
      "88 88 88\n",
      "89 89 89\n",
      "90 fizzbuzz fizzbuzz\n",
      "91 91 91\n",
      "92 92 92\n",
      "93 fizz fizz\n",
      "94 94 94\n",
      "95 fizz buzz\n",
      "96 fizz fizz\n",
      "97 97 97\n",
      "98 98 98\n",
      "99 fizz fizz\n",
      "100 buzz buzz\n",
      "96 / 100\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "\n",
    "print(num_correct, \"/\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥 러닝 (Deep Learning)\n",
    "* Deep Neural Networks (DNN)\n",
    "* Input Size가 크다, Output Size도 크다, 여러 층의 Hidden Layer, 많은 수의 Neuron\n",
    "* Large network &rarr; 많은 컴퓨팅 자원 필요\n",
    "* H/W 발전 + 컴퓨팅 기술 발전 + ML 기술 발전 &rarr; 딥 러닝 가능 &rarr; 복잡하고 어려운 문제도 풀 수 있게 됨\n",
    "* 여러 가지 발전된 모델들\n",
    "    + Convolutional Deep Neural Networks(CDNN)\n",
    "    + Recurrent Neural Networks (RNN)\n",
    "    + Deep Belief Networks (DBN)\n",
    "* 좋은 구현 도구들\n",
    "    + TensorFlow, Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
