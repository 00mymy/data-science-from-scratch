{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "* Classifier\n",
    "  + 카테고리 예측 (Yes/No, Class C1/C2/C3...)\n",
    "* Tree 형태의 예측기\n",
    "  + 대표적인 White-box 모델 (예측 과정과 이유를 투명하게 제공한다)\n",
    "  + 모델을 이해하기 쉽다\n",
    "* 참고: [wiki - 결정 트리](https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Decision Tree\n",
    "* 질문(판단) &rarr; 분기 &rarr; 하위 질문 &rarr; 분기 &rarr; ...\n",
    "  + \"스무고개\" 퀴즈와 유사\n",
    "* Example\n",
    "  + \"I am thinking of an animal\"\n",
    "  + <span style=\"color:blue\">\"Does it have more than five legs?\"</span>\n",
    "  + \"No.\"\n",
    "  + <span style=\"color:blue\">\"Is it delicious?\"</span>\n",
    "  + \"No.\"\n",
    "  + <span style=\"color:blue\">\"Does it appear on the back of the Australian five-cent coin?\"</span>\n",
    "  + \"Yes\"\n",
    "  + <span style=\"color:blue\">\"Is it an enchidna?\"</span>\n",
    "  + \"Yes, it is!\"\n",
    "* 학습을 통해 얻는 지식(모델)을 트리 형태로 관리\n",
    "  + 트리를 따라 내려가면서 판단/분기 &rarr; 예측\n",
    "  + Node: 분기 조건\n",
    "  + Link: Yes/No\n",
    "* Example Tree\n",
    "  + <span style=\"color:blue\">More than 5 legs?</span>\n",
    "    - no\n",
    "      + <span style=\"color:blue\">Delicious?</span>\n",
    "        - no\n",
    "          + <span style=\"color:blue\">On bank of Australian 5-cent coin?</span>\n",
    "            - no\n",
    "              + Kitty cat!\n",
    "            - yes\n",
    "              + Echidra\n",
    "        - yes\n",
    "          + <span style=\"color:blue\">Star of <i>Charlotte's Web</i>?</span>\n",
    "            - no\n",
    "              + <span style=\"color:blue\">Bison!</span>\n",
    "            - yes\n",
    "              + <span style=\"color:blue\">Pig!</span>\n",
    "    - yes\n",
    "      + <span style=\"color:blue\">Is hiding under your bed?</span>\n",
    "        - no\n",
    "          + <span style=\"color:blue\">Makes honey?</span>\n",
    "            - no\n",
    "              + <span style=\"color:blue\">Mosquito!</span>\n",
    "            - yes\n",
    "              + <span style=\"color:blue\">Honeybee!</span>\n",
    "        - yes\n",
    "          + <span style=\"color:blue\">Star of <i>Charlotte's Web</i>?</span>\n",
    "            - no\n",
    "              + <span style=\"color:blue\">Bed bug!</span>\n",
    "            - yes\n",
    "              + <span style=\"color:blue\">Spider!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제의 핵심\n",
    "* \"좋은 질문\"을 해야한다!\n",
    "* 질문의 순서가 중요하다!\n",
    "  + 어떤 조건을 먼저 체크할 것인가?\n",
    "  + 즉, 입력 데이터의 어떤 속성을 먼저 고려할 것인가?\n",
    "* 각 분기 노드에서 속성을 선택하기 위한 합리적이고 견고한, 정량적인 판단 기준이 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "* \"좋은 질문\"을 판단하기 위한 수단\n",
    "  + choose questions whose answers give a lot of information what our tree should predict\n",
    "  + **Entropy** 측정\n",
    "    - Entropy가 높다 $\\sim$ 불확실성이 크다 $\\sim$ 판단의 기준(질문)으로 적합하지 않다\n",
    "* Entropy = $\\sum_{i=1}^C-p_i*\\log_2(p_i)$\n",
    "  + $p_i$: proportion of data labeled as class  $C_i$\n",
    "* Entropy 특성\n",
    "  + 예) 두 개의 Class $C_1$, $C_2$가 있을 때\n",
    "    - $p_1$: 데이터 포인트가 $C_1$에 속할 확률\n",
    "    - $p_2$: 데이터 포인트가 $C_2$에 속할 확률 (= $1-p_1$)\n",
    "  + $p_1=0$ ($p_2=1$) 또는 $p_1=1$ ($p_2=0$)\n",
    "    - Entropy=0\n",
    "    - 불확실성이 없다\n",
    "  + $p_1=p_2=0.5$\n",
    "    - Entropy=1\n",
    "    - 불확실성 최대\n",
    "* 참고: [Entory, Gini, and Information Gain](https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# C1, C2 두 개의 클래스(카테고리)가 있을 때\n",
    "# p1의 값에 따라 엔트로피가 어떻게 달라지는가 그려보자\n",
    "x = [f for f in np.arange(0.01,1, 0.01)]\n",
    "y1 = [-xi*math.log(xi,2) for xi in x]\n",
    "y2 = [-xi*math.log(xi,2) -(1-xi)*math.log((1-xi),2) for xi in x]\n",
    "\n",
    "plt.plot(x,y1, label='-p*log(p)')\n",
    "plt.plot(x,y2, label='sum(-pi*log(pi))')\n",
    "plt.xlabel('p')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy 구해보기 - 연습\n",
    "### 누가 군대에 갈까요?\n",
    "* 전체 100명\n",
    "  + (성별) 남자 : 50, 여자 : 50\n",
    "  + (몸무게)  60Kg 이상 : 60,  60Kg 이하 : 40\n",
    "* 100명 중 군대간 사람은 40명\n",
    "  + 40명 모두 남자\n",
    "  + 40명 중 30명은 몸무게 60Kg 이상\n",
    "* \"성별=='남'\"일 때\n",
    "  + 군대간 사람: 40, 안간사람:10\n",
    "  + 군대갈 확률 : 40/50\n",
    "  + 군대 안갈 확률 : 10/50\n",
    "  + entropy([0.8, 0.2]) = $-0.8log_2{0.8} -0.2log_2{0.2}$ = 0.72\n",
    "* \"몸무게=='60Kg이상'\"일 때\n",
    "  + 군대간 사람: 30, 안간사람:30\n",
    "  + 군대갈 확률 : 30/60\n",
    "  + 군대 안갈 확률 : 30/60\n",
    "  + entropy([0.5, 0.5]) = $-0.5log_2{0.5} -0.5log_2{0.5}$ = 1\n",
    "* 어떤걸  먼저 물어볼까?\n",
    "  + \"**성별**\" vs. \"몸무게\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "\n",
    "def entropy(class_probabilities: List[float]) -> float:\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2)\n",
    "               for p in class_probabilities\n",
    "               if p > 0)                     # ignore zero probabilities\n",
    "\n",
    "assert entropy([1.0]) == 0\n",
    "assert entropy([0.5, 0.5]) == 1\n",
    "assert 0.81 < entropy([0.25, 0.75]) < 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([.8,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([.5,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Entropy 구하기\n",
    "* 주어진 데이터셋의 엔트로피 구하기\n",
    "    + 훈련용 데이터셋의 부분집합 (예, \"성별=남자\"인 데이터셋)\n",
    "* 각 Class(Label,카테고리)에 속할 확률을 구하면 해당 데이터셋의 Entropy를 계산할 수 있다\n",
    "    + 예) \"성별=남자\"인 데이터셋에서 \"군대에 갈 확률\", \"군대에 안 갈 확률\" 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "def class_probabilities(labels: List[Any]) -> List[float]:\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labels: List[Any]) -> float:\n",
    "    return entropy(class_probabilities(labels))\n",
    "\n",
    "assert data_entropy(['a']) == 0\n",
    "assert data_entropy([True, False]) == 1\n",
    "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Entropy of a Partition\n",
    "* 어떤 Feature(질문)을 선택할 것인가 판단하기 위함\n",
    "* Partition $\\sim$ Subtree\n",
    "  + 예) \"성별\"을 가지고 나눈다면\n",
    "    - Subsets : 남자, 여자\n",
    "* $H=q_1H(S_1) + q_2H(S_2) +...+q_mH(S_m)$\n",
    "  + $H$: Partition Entory\n",
    "    - 예) \"성별\"을 기준으로 했을 때의 Entropy\n",
    "  + $H(S_i)$: Data Entropy\n",
    "    - 예) \"성별=남자\"인 데이터셋의 Entropy\n",
    "  + $q_i$: 해당 Partion($S$)에서 데이터셋($S_i$)의 비율\n",
    "    - 예) \"성별=남자\"의 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
    "    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "               for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Decision Tree\n",
    "### 서류전형 자동 분류기\n",
    "* Example Dataset\n",
    "  + 채용 인터뷰 데이터\n",
    "  + Features\n",
    "    - 지원자 등급(level), 주로 사용하는 언어(lang), 트위터 사용여부(tweets), 박사학위 여부(phd), 인터뷰결과(did_well)\n",
    "    - 인터뷰결과(did_well): 예측하고자 하는 값(Class/Category/Label, 종속변수, Dependent Variable)\n",
    "* 알고리즘\n",
    "  1. 각 Feature에 대해 Partition Entropy를 구한다\n",
    "  2. Partition Entropy가 가장 작은 Feature를 선택한다. (Node)\n",
    "  3. 선택된 Feature의 값을 기준으로 분기한다. (Edge)\n",
    "  4. 분기된 각 데이터셋(Subset)에 대해 1~3 과정을 반복하면서 Subtree를 만들어 나간다\n",
    "     + 이 때, 먼저 선택한 Feature는 제외한다\n",
    "  5. 반복 과정에서 분기된 데이터셋이 모든 같은 Label을 가지면 더 이상 분기하지 않는다\n",
    "* 참고: [ID3 Algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypeVar\n",
    "from collections import defaultdict\n",
    "\n",
    "T = TypeVar('T')  # generic type for inputs\n",
    "\n",
    "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
    "    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
    "    partitions: Dict[Any, List[T]] = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = getattr(input, attribute)  # value of the specified attribute\n",
    "        partitions[key].append(input)    # add input to the correct partition\n",
    "    return partitions\n",
    "\n",
    "def partition_entropy_by(inputs: List[Any],\n",
    "                         attribute: str,\n",
    "                         label_attribute: str) -> float:\n",
    "    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n",
    "    # partitions consist of our inputs\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "\n",
    "    # but partition_entropy needs just the class labels\n",
    "    labels = [[getattr(input, label_attribute) for input in partition]\n",
    "              for partition in partitions.values()]\n",
    "\n",
    "    return partition_entropy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "\n",
    "class Candidate(NamedTuple):\n",
    "    level: str\n",
    "    lang: str\n",
    "    tweets: bool\n",
    "    phd: bool\n",
    "    did_well: Optional[bool] = None  # allow unlabeled data\n",
    "\n",
    "                  #  level     lang     tweets  phd  did_well\n",
    "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
    "          Candidate('Senior', 'Java',   False, True,  False),\n",
    "          Candidate('Mid',    'Python', False, False, True),\n",
    "          Candidate('Junior', 'Python', False, False, True),\n",
    "          Candidate('Junior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'R',      True,  True,  False),\n",
    "          Candidate('Mid',    'R',      True,  True,  True),\n",
    "          Candidate('Senior', 'Python', False, False, False),\n",
    "          Candidate('Senior', 'R',      True,  False, True),\n",
    "          Candidate('Junior', 'Python', True,  False, True),\n",
    "          Candidate('Senior', 'Python', True,  True,  True),\n",
    "          Candidate('Mid',    'Python', False, True,  True),\n",
    "          Candidate('Mid',    'Java',   True,  False, True),\n",
    "          Candidate('Junior', 'Python', False, True,  False)\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree - 손으로 그려보자\n",
    "* 어떤 Feature를 먼저 선택할 것인가?\n",
    "  + 각 Feature에 대해 Partition Entropy를 구한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (node)'level'\n",
    "* 가장 작은 Partition Entropy: 'level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[Candidate(level='Senior', lang='Java', tweets=False, phd=False, did_well=False), Candidate(level='Senior', lang='Java', tweets=False, phd=True, did_well=False), Candidate(level='Senior', lang='Python', tweets=False, phd=False, did_well=False), Candidate(level='Senior', lang='R', tweets=True, phd=False, did_well=True), Candidate(level='Senior', lang='Python', tweets=True, phd=True, did_well=True)], [Candidate(level='Mid', lang='Python', tweets=False, phd=False, did_well=True), Candidate(level='Mid', lang='R', tweets=True, phd=True, did_well=True), Candidate(level='Mid', lang='Python', tweets=False, phd=True, did_well=True), Candidate(level='Mid', lang='Java', tweets=True, phd=False, did_well=True)], [Candidate(level='Junior', lang='Python', tweets=False, phd=False, did_well=True), Candidate(level='Junior', lang='R', tweets=True, phd=False, did_well=True), Candidate(level='Junior', lang='R', tweets=True, phd=True, did_well=False), Candidate(level='Junior', lang='Python', tweets=True, phd=False, did_well=True), Candidate(level='Junior', lang='Python', tweets=False, phd=True, did_well=False)]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions = partition_by(inputs, 'level')\n",
    "partitions.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Junior', True),\n",
       " ('Junior', True),\n",
       " ('Junior', False),\n",
       " ('Junior', True),\n",
       " ('Junior', False),\n",
       " ('Mid', True),\n",
       " ('Mid', True),\n",
       " ('Mid', True),\n",
       " ('Mid', True),\n",
       " ('Senior', False),\n",
       " ('Senior', False),\n",
       " ('Senior', False),\n",
       " ('Senior', True),\n",
       " ('Senior', True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_level = [(i.level, i.did_well) for i in inputs]\n",
    "by_level.sort(key=lambda x: x[0])\n",
    "by_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'level' 을 기준으로 분기했을 때\n",
    "  + 'Junior': True 3, False 2\n",
    "  + 'Mid': True 4, False 0 (모두 같은  클래스에 속하므로 더 이상 분기하지 않는다)\n",
    "  + 'Senior': True 2, False 3\n",
    "* 만약 'level'만 가지고 인터뷰결과를 예측한다면?\n",
    "  + IF 'Junior' THEN True, ELSE IF 'Mid' THEN True, ELSE False\n",
    "* 'Junior'와 'Senior'는 한 단계 더 내려가 보면 좋겠다\n",
    "  + 먼저 'Senior'에 대해 한 단계 내려가 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (node)'level' &rarr; (edge)level='Senior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_inputs = [input for input in inputs if input.level == 'Senior']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나머지 Features 중에서 어떤 Feature를 선택할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang','tweets','phd']: # 'level' 제외\n",
    "    print(key, partition_entropy_by(senior_inputs, key, 'did_well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (node)'level' &rarr; (edge)level='Senior' &rarr;  (node)'tweets'\n",
    "* 가장 작은 Partition Entropy: 'tweets'\n",
    "* 'tweets' 값을 기준으로 분기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, False), (False, False), (False, False), (True, True), (True, True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_tweets = [(i.tweets, i.did_well) for i in senior_inputs]\n",
    "by_tweets.sort(key=lambda x: x[0])\n",
    "by_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'tweets'를 기준으로 분기했을 때\n",
    "  + False : True 0, False 3\n",
    "  + True : True 2, False 0\n",
    "  + 두 경우 모두 같은 클래스에 속하므로 더 이상 분기하지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (node)'level' &rarr; (edge)level='Junior'\n",
    "* 'Junior'에 대해서도 위와 같은 방식으로 Subtree를 그릴 수 있다\n",
    "\n",
    "### 최종 결과 (Decision Tree)\n",
    "* **level?**\n",
    "  + <span style=\"color:blue\">Senior</span>\n",
    "    - **tweets?**\n",
    "      + <span style=\"color:blue\">Yes</span>\n",
    "        - **HIRE!**\n",
    "      + <span style=\"color:blue\">No</span>\n",
    "        - **DO NOT HIRE!**\n",
    "  + <span style=\"color:blue\">Mid</span>\n",
    "    - **HIRE!**\n",
    "  + <span style=\"color:blue\">Junior</span>\n",
    "    - **phd?**\n",
    "      + <span style=\"color:blue\">No</span>\n",
    "        - **HIRE!**\n",
    "      + <span style=\"color:blue\">Yes</span>\n",
    "        - **DO NOT HIRE!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "* 학습\n",
    "  + Training Dataset을 이용하여 Decision Tree 만들기\n",
    "* 예측\n",
    "  + Decision Tree 탐색하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 자료 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Union, Any\n",
    "\n",
    "class Leaf(NamedTuple):\n",
    "    value: Any\n",
    "\n",
    "class Split(NamedTuple):\n",
    "    attribute: str\n",
    "    subtrees: dict\n",
    "    default_value: Any = None\n",
    "\n",
    "DecisionTree = Union[Leaf, Split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 클래스로 앞에서 구한 서류전형 자동 분류기 트리를 표현하면 다음과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiring_tree = Split('level', {   # First, consider \"level\".\n",
    "    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n",
    "        False: Leaf(True),       #   if \"phd\" is False, predict True\n",
    "        True: Leaf(False)        #   if \"phd\" is True, predict False\n",
    "    }),\n",
    "    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n",
    "    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n",
    "        False: Leaf(False),      #   if \"tweets\" is False, predict False\n",
    "        True: Leaf(True)         #   if \"tweets\" is True, predict True\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "* Decision Tree를 이용한 예측기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree: DecisionTree, input: Any) -> Any:\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # If this is a leaf node, return its value\n",
    "    if isinstance(tree, Leaf):\n",
    "        return tree.value\n",
    "\n",
    "    # Otherwise this tree consists of an attribute to split on\n",
    "    # and a dictionary whose keys are values of that attribute\n",
    "    # and whose values of are subtrees to consider next\n",
    "    subtree_key = getattr(input, tree.attribute)\n",
    "\n",
    "    if subtree_key not in tree.subtrees:   # If no subtree for key,\n",
    "        return tree.default_value          # return the default value.\n",
    "\n",
    "    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n",
    "    return classify(subtree, input)        # and use it to classify the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "* Decision Tree 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs: List[Any],\n",
    "                   split_attributes: List[str],\n",
    "                   target_attribute: str) -> DecisionTree:\n",
    "    # Count target labels\n",
    "    label_counts = Counter(getattr(input, target_attribute)\n",
    "                           for input in inputs)\n",
    "    most_common_label = label_counts.most_common(1)[0][0]\n",
    "\n",
    "    # If there's a unique label, predict it\n",
    "    if len(label_counts) == 1:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # If no split attributes left, return the majority label\n",
    "    if not split_attributes:\n",
    "        return Leaf(most_common_label)\n",
    "\n",
    "    # Otherwise split by the best attribute\n",
    "\n",
    "    def split_entropy(attribute: str) -> float:\n",
    "        \"\"\"Helper function for finding the best attribute\"\"\"\n",
    "        return partition_entropy_by(inputs, attribute, target_attribute)\n",
    "\n",
    "    best_attribute = min(split_attributes, key=split_entropy)\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_attributes = [a for a in split_attributes if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = {attribute_value : build_tree_id3(subset,\n",
    "                                                 new_attributes,\n",
    "                                                 target_attribute)\n",
    "                for attribute_value, subset in partitions.items()}\n",
    "\n",
    "    return Split(best_attribute, subtrees, default_value=most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs,\n",
    "                      ['level', 'lang', 'tweets', 'phd'],\n",
    "                      'did_well')\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Junior\", \"Java\", True, False))\n",
    "\n",
    "# Should predict False\n",
    "assert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n",
    "\n",
    "# Should predict True\n",
    "assert classify(tree, Candidate(\"Intern\", \"Java\", True, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "* Decision Tree를 여러개 만든다\n",
    "  + 예) Training Dataset을 각각 다르게 구성하여 학습\n",
    "  + 참고: bootstrap_sample(inputs)\n",
    "* 여러 Decision Tree의 결과 → Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 'scratch' package가 설치된 디렉토리로 작업 디렉토리 바꿈 (package를 import하기 위함)\n",
    "import os, sys\n",
    "os.chdir('..')\n",
    "\n",
    "from scratch.multiple_regression import bootstrap_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def build_forest(inputs, num_trees=3, num_samples=10):\n",
    "    trees = []\n",
    "    for i in range(num_trees):\n",
    "        #tree = build_tree_id3(random.sample(inputs, num_samples))\n",
    "        tree = build_tree_id3(bootstrap_sample(inputs), ['level', 'lang', 'tweets', 'phd'], 'did_well')\n",
    "        trees.append(tree)\n",
    "    \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = build_forest(inputs, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior / Java / tweets / phd ==> True\n"
     ]
    }
   ],
   "source": [
    "print(\"Junior / Java / tweets / phd ==>\", classify(tree, Candidate(\"Junior\", \"Java\", True, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior / Java / tweets / phd ==> True\n"
     ]
    }
   ],
   "source": [
    "print(\"Junior / Java / tweets / phd ==>\", forest_classify(trees, Candidate(\"Junior\", \"Java\", True, False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Further Exploration\n",
    "* ID3\n",
    "  + Categorical features\n",
    "* C4.5\n",
    "  + Categorical & Numerical\n",
    "* CART\n",
    "  + Categorical & Numerical, Classification & Regression\n",
    "  + *scikit-learn uses an optimised version of the CART algorithm*\n",
    "  \n",
    "scikit-learn: [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
